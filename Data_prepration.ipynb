{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31aa048f",
   "metadata": {},
   "source": [
    "Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "fc95cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "dd3752ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt_tab')\n",
    "# Force download to the correct path\n",
    "nltk.download('punkt', download_dir=r\"C:\\Users\\anshk\\AppData\\Roaming\\nltk_data\")\n",
    "\n",
    "# Append the correct nltk data path\n",
    "nltk.data.path.append(r\"C:\\Users\\anshk\\AppData\\Roaming\\nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c3031170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>urgency_level</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Payment issue for my SmartWatch V2. I was unde...</td>\n",
       "      <td>Billing Problem</td>\n",
       "      <td>Medium</td>\n",
       "      <td>SmartWatch V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Can you tell me more about the UltraClean Vacu...</td>\n",
       "      <td>General Inquiry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UltraClean Vacuum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>I ordered SoundWave 300 but got EcoBreeze AC i...</td>\n",
       "      <td>Wrong Item</td>\n",
       "      <td>Medium</td>\n",
       "      <td>SoundWave 300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Facing installation issue with PhotoSnap Cam. ...</td>\n",
       "      <td>Installation Issue</td>\n",
       "      <td>Low</td>\n",
       "      <td>PhotoSnap Cam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Order #30903 for Vision LED TV is 13 days late...</td>\n",
       "      <td>Late Delivery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vision LED TV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                        ticket_text  \\\n",
       "0          1  Payment issue for my SmartWatch V2. I was unde...   \n",
       "1          2  Can you tell me more about the UltraClean Vacu...   \n",
       "2          3  I ordered SoundWave 300 but got EcoBreeze AC i...   \n",
       "3          4  Facing installation issue with PhotoSnap Cam. ...   \n",
       "4          5  Order #30903 for Vision LED TV is 13 days late...   \n",
       "\n",
       "           issue_type urgency_level            product  \n",
       "0     Billing Problem        Medium      SmartWatch V2  \n",
       "1     General Inquiry           NaN  UltraClean Vacuum  \n",
       "2          Wrong Item        Medium      SoundWave 300  \n",
       "3  Installation Issue           Low      PhotoSnap Cam  \n",
       "4       Late Delivery           NaN      Vision LED TV  "
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('dataset.xls')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "96882be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8c56b192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "issue_type       76\n",
       "ticket_text      55\n",
       "urgency_level    52\n",
       "ticket_id         0\n",
       "product           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "409c08ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticket_id        0\n",
       "ticket_text      0\n",
       "issue_type       0\n",
       "urgency_level    0\n",
       "product          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "f566ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate = df[df.duplicated()]\n",
    "# print(duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "1f2fd323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop_duplicates(keep=False)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "05d3a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z0-9\\s]', '', text)  # Remove punctuation\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "#     return text\n",
    "\n",
    "# df['ticket_text'] = df['ticket_text'].apply(clean_text)\n",
    "# df['issue_type'] = df['issue_type'].apply(clean_text)\n",
    "# df['urgency_level'] = df['urgency_level'].apply(clean_text)\n",
    "# df['product'] = df['product'].apply(clean_text)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "c138ab4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticket_id</th>\n",
       "      <th>ticket_text</th>\n",
       "      <th>issue_type</th>\n",
       "      <th>urgency_level</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>payment issue smartwatch v underbilled order</td>\n",
       "      <td>billing problem</td>\n",
       "      <td>medium</td>\n",
       "      <td>SmartWatch V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ordered soundwave got ecobreeze ac instead ord...</td>\n",
       "      <td>wrong item</td>\n",
       "      <td>medium</td>\n",
       "      <td>SoundWave 300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>facing installation issue photosnap cam setup ...</td>\n",
       "      <td>installation issue</td>\n",
       "      <td>low</td>\n",
       "      <td>PhotoSnap Cam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>tell photosnap cam warranty also available red</td>\n",
       "      <td>general inquiry</td>\n",
       "      <td>medium</td>\n",
       "      <td>PhotoSnap Cam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>malfunction stopped working day</td>\n",
       "      <td>product defect</td>\n",
       "      <td>low</td>\n",
       "      <td>EcoBreeze AC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ticket_id                                        ticket_text  \\\n",
       "0          1       payment issue smartwatch v underbilled order   \n",
       "2          3  ordered soundwave got ecobreeze ac instead ord...   \n",
       "3          4  facing installation issue photosnap cam setup ...   \n",
       "5          6     tell photosnap cam warranty also available red   \n",
       "6          7                    malfunction stopped working day   \n",
       "\n",
       "           issue_type urgency_level        product  \n",
       "0     billing problem        medium  SmartWatch V2  \n",
       "2          wrong item        medium  SoundWave 300  \n",
       "3  installation issue           low  PhotoSnap Cam  \n",
       "5     general inquiry        medium  PhotoSnap Cam  \n",
       "6      product defect           low   EcoBreeze AC  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['ticket_text'] = df['ticket_text'].apply(preprocess_text)\n",
    "df['issue_type'] = df['issue_type'].apply(preprocess_text)\n",
    "df['urgency_level'] = df['urgency_level'].apply(preprocess_text)\n",
    "df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "fc573255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(df['ticket_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "f4da16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "df['text_length'] = df['ticket_text'].apply(len)\n",
    "df['sentiment'] = df['ticket_text'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "62b9efe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\anshk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER sentiment lexicon (only once)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to extract sentiment score\n",
    "def get_sentiment_score(text):\n",
    "    sentiment = sia.polarity_scores(text)\n",
    "    return sentiment['compound']  # Range: -1 (negative) to +1 (positive)\n",
    "\n",
    "# Function to map sentiment score to urgency level\n",
    "def predict_urgency_from_sentiment(text):\n",
    "    score = get_sentiment_score(text)\n",
    "    \n",
    "    if score <= -0.4:\n",
    "        return \"high\"\n",
    "    elif score <= 0.2:\n",
    "        return \"medium\"\n",
    "    else:\n",
    "        return \"low\"\n",
    "\n",
    "# Apply to your DataFrame\n",
    "df['sentiment_score'] = df['ticket_text'].apply(get_sentiment_score)\n",
    "df['predicted_urgency'] = df['ticket_text'].apply(predict_urgency_from_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "d3f93732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "X_combined = hstack([X_tfidf, \n",
    "                     np.array(df['text_length']).reshape(-1, 1), \n",
    "                     np.array(df['sentiment']).reshape(-1, 1)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d7c44aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "issue_encoder = LabelEncoder()\n",
    "urgency_encoder = LabelEncoder()\n",
    "\n",
    "y_issue = issue_encoder.fit_transform(df['issue_type'])\n",
    "y_urgency = urgency_encoder.fit_transform(df['urgency_level'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "0e5ce4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue Type Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "    account access       1.00      1.00      1.00        23\n",
      "   billing problem       1.00      1.00      1.00        19\n",
      "   general inquiry       1.00      1.00      1.00        25\n",
      "installation issue       1.00      1.00      1.00        29\n",
      "     late delivery       1.00      1.00      1.00        17\n",
      "    product defect       1.00      1.00      1.00        30\n",
      "        wrong item       1.00      1.00      1.00        23\n",
      "\n",
      "          accuracy                           1.00       166\n",
      "         macro avg       1.00      1.00      1.00       166\n",
      "      weighted avg       1.00      1.00      1.00       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train, X_test, y_issue_train, y_issue_test = train_test_split(X_combined, y_issue, test_size=0.2, random_state=42)\n",
    "\n",
    "issue_model = LogisticRegression(max_iter=10000)\n",
    "issue_model.fit(X_train, y_issue_train)\n",
    "y_issue_pred = issue_model.predict(X_test)\n",
    "\n",
    "print(\"Issue Type Classification Report:\")\n",
    "print(classification_report(y_issue_test, y_issue_pred, target_names=issue_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "487700f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Urgency Level Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        high       0.40      0.35      0.37        66\n",
      "         low       0.29      0.33      0.31        43\n",
      "      medium       0.32      0.33      0.32        57\n",
      "\n",
      "    accuracy                           0.34       166\n",
      "   macro avg       0.33      0.34      0.33       166\n",
      "weighted avg       0.34      0.34      0.34       166\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(X_combined, y_urgency, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "urgency_model = LogisticRegression(max_iter=10000)\n",
    "urgency_model.fit(X_train_u, y_train_u)\n",
    "y_pred_u = urgency_model.predict(X_test_u)\n",
    "\n",
    "print(\"Urgency Level Classification Report:\")\n",
    "print(classification_report(y_test_u, y_pred_u, target_names=urgency_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "172d5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    # Predefined products and complaint keywords\n",
    "    products_list = [\n",
    "        \"SmartWatch V2\", \"UltraClean Vacuum\", \"SoundWave 300\", \"PhotoSnap Cam\", \"Vision LED TV\",\n",
    "        \"EcoBreeze AC\", \"RoboChef Blender\", \"FitRun Treadmill\", \"PowerMax Battery\", \"ProTab X1\"\n",
    "    ]\n",
    "    complaint_keywords_list = [\n",
    "    \"no response\", \"charged twice\", \"showing blocked\", \"not working\", \"setup fails\",\n",
    "    \"not received\", \"wrong item\", \"debited incorrectly\", \"not refunded\", \"mixed up\",\n",
    "    \"captcha failed\", \"unknown issue\", \"reset required\", \"not able to install\", \"can’t log in\",\n",
    "    \"can’t access\"\n",
    "]\n",
    "\n",
    "    # Method 1: Predefined list matching\n",
    "    found_products = [p for p in products_list if p.lower() in text.lower()]\n",
    "    found_keywords = [kw for kw in complaint_keywords_list if kw in text.lower()]\n",
    "    dates = re.findall(r'\\d{1,2} [A-Za-z]+', text)\n",
    "\n",
    "    # Method 2: spaCy NER and custom rules\n",
    "    doc = nlp(text)\n",
    "    ner_products = []\n",
    "    ner_complaint_phrases = []\n",
    "\n",
    "    # Extract products using NER\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PRODUCT\", \"ORG\"]:\n",
    "            ner_products.append(ent.text)\n",
    "\n",
    "    # Extract complaint keywords using spaCy\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\" and token.pos_ in [\"VERB\", \"ADJ\"]:\n",
    "            if token.lemma_.lower() in complaint_keywords_list:\n",
    "                ner_complaint_phrases.append(token.text)\n",
    "\n",
    "    # Combine results\n",
    "    products = list(set(found_products)) if found_products else list(set(ner_products))\n",
    "    complaint_keywords = list(set(found_keywords + ner_complaint_phrases))\n",
    "    dates = dates if dates else [\"No dates found\"]\n",
    "\n",
    "    return {\n",
    "        \"products\": products,\n",
    "        \"dates\": dates,\n",
    "        \"complaint_keywords\": complaint_keywords\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "4ebcda80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_ticket(text):\n",
    "    try:\n",
    "        preprocessed = preprocess_text(text)\n",
    "        tfidf_input = tfidf.transform([preprocessed])\n",
    "        text_length = len(text)\n",
    "        sentiment = TextBlob(text).sentiment.polarity\n",
    "\n",
    "        combined_features = hstack([tfidf_input, [[text_length]], [[sentiment]]])\n",
    "\n",
    "        issue_pred = issue_encoder.inverse_transform(issue_model.predict(combined_features))[0]\n",
    "        urgency_pred = urgency_encoder.inverse_transform(urgency_model.predict(combined_features))[0]\n",
    "        entities = extract_entities(text)\n",
    "\n",
    "        return {\n",
    "            \"predicted_issue_type\": issue_pred,\n",
    "            \"predicted_urgency_level\": urgency_pred,\n",
    "            \"entities\": entities\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"predicted_issue_type\": \"Error: \" + str(e),\n",
    "            \"predicted_urgency_level\": \"Error: \" + str(e),\n",
    "            \"entities\": \"Error: \" + str(e)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "daa26174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7891\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7891/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: {'predicted_issue_type': 'billing problem', 'predicted_urgency_level': 'low', 'entities': {'products': ['EcoBreeze AC'], 'dates': ['No dates found'], 'complaint_keywords': ['charged twice']}}\n",
      "DEBUG: {'predicted_issue_type': 'product defect', 'predicted_urgency_level': 'low', 'entities': {'products': ['FitRun Treadmill'], 'dates': ['2 days'], 'complaint_keywords': []}}\n",
      "DEBUG: {'predicted_issue_type': 'wrong item', 'predicted_urgency_level': 'high', 'entities': {'products': [], 'dates': ['12 days', '15 May'], 'complaint_keywords': ['mixed up']}}\n",
      "DEBUG: {'predicted_issue_type': 'wrong item', 'predicted_urgency_level': 'high', 'entities': {'products': ['UltraClean Vacuum'], 'dates': ['15 March'], 'complaint_keywords': ['not received', 'not working']}}\n",
      "DEBUG: {'predicted_issue_type': 'wrong item', 'predicted_urgency_level': 'high', 'entities': {'products': ['UltraClean Vacuum'], 'dates': ['15 March'], 'complaint_keywords': ['not received', 'not working']}}\n",
      "DEBUG: {'predicted_issue_type': 'billing problem', 'predicted_urgency_level': 'low', 'entities': {'products': ['Freshly Foundations Vacuum'], 'dates': ['01 July'], 'complaint_keywords': ['charged twice', 'can’t log in', 'showing blocked']}}\n",
      "DEBUG: {'predicted_issue_type': 'billing problem', 'predicted_urgency_level': 'low', 'entities': {'products': ['Freshly Foundations Vacuum'], 'dates': ['01 July'], 'complaint_keywords': ['charged twice', 'can’t log in', 'showing blocked']}}\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def gradio_interface(ticket_text):\n",
    "    result = process_ticket(ticket_text)\n",
    "\n",
    "    # Print to console for full debugging\n",
    "    print(\"DEBUG:\", result)\n",
    "\n",
    "    return (\n",
    "        result['predicted_issue_type'],\n",
    "        result['predicted_urgency_level'],\n",
    "        result['entities']\n",
    "    )\n",
    "\n",
    "    # return {\n",
    "    #     \"Predicted Issue Type\": result['predicted_issue_type'],\n",
    "    #     \"Predicted Urgency Level\": result['predicted_urgency_level'],\n",
    "    #     \"Extracted Entities\": result['entities']\n",
    "    # }\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=gr.Textbox(lines=5, label=\"Enter Ticket Text\"),\n",
    "    outputs=[\n",
    "        gr.Text(label=\"Predicted Issue Type\"),\n",
    "        gr.Text(label=\"Predicted Urgency Level\"),\n",
    "        gr.JSON(label=\"Extracted Entities\")\n",
    "    ],\n",
    "    title=\"Customer Ticket Classifier\",\n",
    "    description=\"Paste a support ticket message to classify its issue type, urgency level, and extract key entities.\"\n",
    ")\n",
    "\n",
    "demo.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
